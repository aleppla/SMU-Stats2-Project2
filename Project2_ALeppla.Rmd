---
title: "R Notebook"
author: "Andrew Leppla"
date: "3/24/2020"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import Data

```{r import}
bank=read.csv("~/R/SMU-Stats2-Project2/bank-additional-full.csv", header=T, sep=";")
```

## Load Libraries

```{r libraries}
suppressMessages(library(tidyverse))
suppressMessages(library(MASS))
suppressMessages(library(caret))
suppressMessages(library(ROCR))
suppressMessages(library(glmnet))
suppressMessages(library(bestglm))
#library(mvtnorm)
```

## EDA

```{r EDA}
summary(bank)
# default has 20% "unknown" missing data, may need to impute
# poutcome has level "nonexistent" for most rows
# housing and loan have 2.4% "unknown" values, probably not a big deal
# job, education, and month all have many levels (>7)

summary(bank$month)
# month only has 10 levels, missing jan and feb
bank %>% ggplot(aes(month,fill=y)) + geom_bar(position="dodge") 
bank %>% ggplot(aes(month,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_train %>% ggplot(aes(month,fill=y)) + geom_bar(position="dodge") 
#bank_train %>% ggplot(aes(month,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_test_all %>% ggplot(aes(month,fill=y)) + geom_bar(position="dodge") 
#bank_test_all %>% ggplot(aes(month,fill=y)) + geom_bar(position="fill") + ylab("Proportion")

summary(bank$day_of_week)
bank %>% ggplot(aes(day_of_week,fill=y)) + geom_bar(position="dodge") 
bank %>% ggplot(aes(day_of_week,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
# No differences in day of week

bank %>% ggplot(aes(pdays,fill=y)) + geom_histogram(position="dodge",binwidth=500)
bank %>% filter(pdays<999) %>% ggplot(aes(pdays,stat(density),fill=y)) + geom_histogram(position="dodge") 
# pdays is mostly 999's with a few values < 30
# Possible predictor of yes vs. no, more binary than continuous

# Is pdays=999 the same as poutcome="nonexistent" ?
pdays999=grepl(999,bank$pdays)
summary(pdays999)
poutcome_nonexistent=grepl("nonexistent",bank$poutcome)
summary(poutcome_nonexistent)
diff=factor(abs(pdays999-poutcome_nonexistent),labels=c("same","different"))
summary(diff) 
# Not quite the same, there are 4,110 rows (1% of data) that are different between them
poutcome_999 = bank$poutcome[diff=="different"] 
summary(poutcome_999)
# All 4,110 of the differing rows are poutcome="failure"
summary(bank$poutcome)
# pdays 999 = poutcome "nonexistent" + "failure"
## Confounded Variables
## Keep poutcome for logistic reg.
## Keep pdays for LDA 

summary(bank$education)
# 4% "unknown" values
bank %>% ggplot(aes(education,fill=y)) + geom_bar(position="dodge")
bank %>% ggplot(aes(education,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
length(grep("illiterate",bank$education)) # n=12
#bank_train %>% ggplot(aes(education,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_test_all %>% ggplot(aes(education,fill=y)) + geom_bar(position="fill") + ylab("Proportion")


summary(bank$job)
# <1% "unknown" values
bank %>% ggplot(aes(job,fill=y)) + geom_bar(position="dodge")
bank %>% ggplot(aes(job,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_train %>% ggplot(aes(job,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_test_all %>% ggplot(aes(job,fill=y)) + geom_bar(position="fill") + ylab("Proportion")

bank %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=11)
bank %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=0.5)
#bank_train %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=10)
#bank_train %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=0.5)

# ?? Recode campaign into ranges: 0-10,10-20,20-30,>40 ??

bank %>% ggplot(aes(previous,fill=y)) + geom_histogram(position="dodge")
bank %>% ggplot(aes(previous,fill=y)) + geom_histogram(position="fill")
#bank_train %>% ggplot(aes(previous,fill=y)) + geom_histogram(position="dodge")
#bank_train %>% ggplot(aes(previous,fill=y)) + geom_histogram(position="fill")


```


```{r EDA Correlations}


# Correlations between continuous predictors

pairs(bank[,c(1,11,23,13,25,16:20)],col=bank$y)
# pdays and log_duration (and maybe age or cons._.idx) looks good for LDA
# lots of repeated values for columns 16-20

cor(bank[,c(1,11,23,13,25,16:20)])
# emp.var.rate, euribor3m, and nr.employed are highly correlated (r>0.9)
# cons.price.idx is also moderately correlated (r= 0.52 to 0.75) 

pairs(bank[,c(1,13,25,16,17,18)],col=bank$y)

cor(bank[,c(1,13,17:20)])
# minimal correlation between age, pdays, and cons._.idx predictors
```


```{r training-test split}

#Split data by yes's and no's

bank_yes=bank[bank$y=="yes",]
bank_no=bank[bank$y=="no",]


#Split yes's into 70/30 training & test sets

down_sample_size = dim(bank_yes)[1]*0.7

set.seed(1113)
index<-sample(1:dim(bank_yes)[1],down_sample_size,replace=F)
train_yes<-bank_yes[index,]
test_yes<-bank_yes[-index,]

dim(test_yes)[1] #1392 rows

#Take same number of no's as yes's for training set

set.seed(3111)
index<-sample(1:dim(bank_no)[1],down_sample_size,replace=F)
train_no<-bank_no[index,]
test_no<-bank_no[-index,] #All remaining no's in test set

# Recombine the yes's and no's into training and test sets for modeling

bank_train = rbind(train_yes,train_no)
bank_test_all = rbind(test_yes,test_no)

# Check that there are no missing variables between the training and test sets

summary(bank_train)
summary(bank_test)

## job, education, and month all have many levels (>7)

summary(bank_train$job)
summary(bank_test$job)

summary(bank_train$education)
summary(bank_test$education)

summary(bank_train$month)
summary(bank_test$month)


# Rebalance back to 90/10 for test set

no_90 = dim(test_yes)[1]/0.1*0.9
no_90 #12,528 rows

index<-sample(1:dim(test_no)[1],no_90,replace=F)
test_no_90_10 = test_no[index,]

bank_test_90_10 = rbind(test_yes,test_no_90_10)
summary(bank_test_90_10$y)[1]/dim(bank_test_90_10)[1] #check = 90% 


# Rebalance to 50/50 for test set

index<-sample(1:dim(test_no)[1],dim(test_yes)[1],replace=F)
test_no_50_50 = test_no[index,]

bank_test_50_50 = rbind(test_yes,test_no_50_50)

########################################################################
# Use test_50_50 as test set
bank_test=bank_test_all

########################################################################
 

#Write CSV files for training & test sets

#write.csv(bank_train,file="~/R/SMU-Stats2-Project2/bank_train_50_50.csv")
#write.csv(bank_test_all,file="~/R/SMU-Stats2-Project2/bank_test_all.csv")
#write.csv(bank_test_90_10,file="~/R/SMU-Stats2-Project2/bank_test_90_10.csv")
#write.csv(bank_test_50_50,file="~/R/SMU-Stats2-Project2/bank_test_50_50.csv")

```


```{r Simple LDA}

# LDA with single best predictor: pdays

## Full Data, No Validation

mylda1 <- lda(y ~ pdays, data = bank)
pred<-predict(mylda1,newdata=bank)
CM1 = confusionMatrix(table(pred$class,bank$y))
CM1 # 90% accuracy

#Plot the simple LDA model

np <- 300
nd.x <- seq(from = min(bank$pdays), to = max(bank$pdays), length.out = np)
nd.y <- seq(from = min(bank$cons.price.idx), to = max(bank$cons.price.idx), length.out = np)
nd <- expand.grid(pdays = nd.x, cons.price.idx = nd.y)

prd <- as.numeric(predict(mylda1, newdata = nd)$class)

plot(bank[, c(13,17)], col = bank$y, main="Simple LDA")
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np), 
        levels = c(1, 2), add = TRUE, drawlabels = FALSE, col="blue")


## Leave-One_out Internal Cross Validation (CV)

### Full Data

mylda1a <- lda(y ~ pdays, data = bank, CV=T)
CM1a = confusionMatrix(table(mylda1a$class,bank$y))
CM1a # 90% accuracy
```


## Include campaign and previous in LDA ??

```{r down-sampling LDA}

## LDA w/ Down-Sampled Training Set

mylda1b <- lda(y ~ pdays, data = bank_train)

### Training Set

pred<-predict(mylda1b,newdata=bank_train)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,2],bank_train$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .40, y = .4,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="Simple LDA - Train ROC Curve")

CM1b = confusionMatrix(table(pred$class,bank_train$y))
CM1b

### Test Set - External Validation

pred<-predict(mylda1b,newdata=bank_test)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,2],bank_test$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .40, y = .4,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="Simple LSA - Test ROC Curve")

CM1c = confusionMatrix(table(pred$class,bank_test$y))
CM1c


# Add all indep. predictors and 2-way interactions to LDA model
# Better ROC Curves, higher AUC

## Full Data Set, Leave-One-Out Interval CV

bank2=bank[,c(1,13,17,18,21)]

mylda2 <- lda(y ~.^2, data = bank2,CV=T)
CM2 = confusionMatrix(table(mylda2$class,bank2$y))
CM2

preds1 <- as.data.frame(mylda2$posterior)
pred1 <- prediction(preds1[,2],bank2$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .40, y = .4,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="LDA Complex Model - Internal CV ROC Curve")

## Training/Test External CV

bank2_train=bank_train[,c(1,13,17,18,21)]
bank2_test=bank_test[,c(1,13,17,18,21)]

mylda2a <- lda(y ~.^2, data = bank2_train)

pred<-predict(mylda2a,newdata=bank2_train)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,2],bank2_train$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .35, y = .5,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="LDA Complex Model - Train Set ROC Curve")

pred<-predict(mylda2a,newdata=bank2_test)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,2],bank2_test$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .35, y = .5,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="LDA Complex Model - Test Set ROC Curve")

CM2 = confusionMatrix(table(pred$class,bank2_test$y))
CM2 # 0.50 Threshold

```


```{r Threshold and Priors}

# Just to have the code

# Change the prediction threshold
pred<-predict(mylda2,newdata=bank2)
thresh <- 0.94
out <- factor(ifelse(pred$posterior[,1] > thresh, "no", "yes"), levels = c("no", "yes"))
confusionMatrix(out,bank2$y)


# Change priors rather than prediction threshold
mylda2p <- lda(y ~.^2, data = bank2,prior=c(0.33,0.67))
pred<-predict(mylda2p,newdata=bank2)$class
CM2p = confusionMatrix(table(pred,bank2$y))
CM2p

```



```{r Obj1 - Log Reg LASSO}

#glmnet requires a matrix 
obj1.train.x <- model.matrix(y~.-1,bank_train[,c(-5,-11,-13,-17,-19,-20)]) 
# Exclude duration, a response/covariate vs. a predictor
# Exclude pdays, use recoded "pcontact" instead
# Exclude economic indicators that are highly collinear

cvfit1 <- cv.glmnet(obj1.train.x, bank_train$y, family = "binomial", type.measure = "class")
plot(cvfit1)
coef(cvfit1) # Elbow, 9 terms in model
coef(cvfit1, s="lambda.min") # Next local min, 17 terms

######################################
# Doesn't maintain variable hierarchy
# Rerun model with GLM ?
######################################

#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit1.pred.train <- predict(cvfit1, newx = obj1.train.x, type = "response",s="lambda.min") 

#Create ROC curves
pred <- prediction(fit1.pred.train[,1], bank_train$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values

#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="Obj1 - Train Set ROC")

#Run model from training set on valid. set 
obj1.test.x = model.matrix(y~.-1,bank_test[,c(-5,-11,-13,-17,-19,-20)])

fit1.pred.test <- predict(cvfit1, newx = obj1.test.x, type = "response",s="lambda.min")

#ROC curves
pred1 <- prediction(fit1.pred.test[,1], bank_test$y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],3), sep = ""))
title(main="Obj1 - Test Set ROC")

```


## Fit the selection model with GLM to get AIC, VIFs, etc.

```{r GLM Logistic}

model.main<-glm(y~job+marital+education+contact+month+campaign+poutcome+emp.var.rate+cons.conf.idx, bank_train, 
                family = binomial(link="logit"))

summary(model.main)

library(car)
(vif(model.main)[,3])^2
#previous VIF=5

library(ResourceSelection)
hoslem.test(model.main$y, fitted(model.main), g=10)
# Sig. lack of fit, but large n

fit1.pred.train <- predict(model.main, newdata = bank_train) 

#Create ROC curves
pred <- prediction(fit1.pred.train, bank_train$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values

#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="Train Set ROC")


#Run model from training set on valid set 
fit1.pred.test <- predict(model.main, newdata = bank_test)

#ROC curves
pred1 <- prediction(fit1.pred.test, bank_test$y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],3), sep = ""))
title(main="Test Set ROC")

```


## Fit a more complex Logistic model with interaction terms 
 
!!! This will take a few minutes to run with all of the categorical interaction terms (800+) !!!

```{r Objective 2}

#glmnet requires a matrix 
dat.train.x <- model.matrix(y~.^2-1,bank_train[,c(-4,-5,-10,-11,-13,-17,-19,-20,-22)])
cvfit <- cv.glmnet(dat.train.x, bank_train$y, family = "binomial", type.measure = "class", nlambda = 30)

plot(cvfit)
coef(cvfit) # 20/350 terms in model
#coef(cvfit,s="lambda.min")

#####################################
# Doesn't maintain variable hierarchy
#####################################

#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit.pred <- predict(cvfit, newx = dat.train.x, type = "response") 

#Compare the prediction to the real outcome
head(fit.pred)
head(bank_train$y)

#Create ROC curves
pred <- prediction(fit.pred[,1], bank_train$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values

#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="Obj2 Training Set ROC")

#Run model from training set on valid set 
dat.test.x = model.matrix(y~.^2-1,bank_test[,c(-4,-5,-10,-11,-13,-17,-19,-20,-22)])

fit.pred1 <- predict(cvfit, newx = dat.test.x, type = "response")

#ROC curves
pred1 <- prediction(fit.pred1[,1], bank_test$y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],3), sep = ""))
title(main="Obj2 Test Set ROC")

```


```{r Compare}

#Here we are just comparing the reproducibility of the logistic model.
#You can also use this code to compare multiple models like logistic, compared to lda, compared to a tree or RF.
#This graph also allows for you to get a sense of what cut off values are producing the best sensitivity and specificity results as well using
#the colorize option.

#This is helpful:  https://www.r-bloggers.com/a-small-introduction-to-the-rocr-package/ that does some extra things you might find 
#helpful.
#If you want to mess around with other packages: https://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/
plot( roc.perf1, colorize = TRUE)
plot(roc.perf2, add = TRUE, colorize = TRUE)
plot(roc.perf3, add = TRUE, colorize = TRUE)
abline(a=0, b= 1)

#without color for cutoff; but adding colors to allow for comarisons of the curves
plot( roc.perf1)
plot(roc.perf2,col="orange", add = TRUE)
plot(roc.perf3,col="blue", add = TRUE)
legend("bottomright",legend=c("Valid 1","Valid 2","Valid 3"),col=c("black","orange","blue"),lty=1,lwd=1)
abline(a=0, b= 1)


```

