---
title: "R Notebook"
author: "Andrew Leppla"
date: "4/16/2020"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import Data

```{r import}
bank=read.csv("~/R/SMU-Stats2-Project2/bank-additional-full.csv", header=T, sep=";")
summary(bank$y)
bank$y=factor(bank$y,levels=c("yes","no"))
summary(bank$y)
```

## Load Libraries

```{r libraries, include=FALSE}
library(tidyverse)
library(MASS)
library(caret)
library(ROCR)
library(glmnet)
library(bestglm)
library(car)
library(ResourceSelection)
library(randomForest)
#library(mvtnorm)
```

## EDA

```{r EDA}
summary(bank)
# default has 20% "unknown" missing data, may need to impute
# poutcome has level "nonexistent" for most rows
# housing and loan have 2.4% "unknown" values, probably not a big deal
# job, education, and month all have many levels (>7)

summary(bank$month)
# month only has 10 levels, missing jan and feb
bank %>% ggplot(aes(month,fill=y)) + geom_bar(position="dodge") 
bank %>% ggplot(aes(month,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_train %>% ggplot(aes(month,fill=y)) + geom_bar(position="dodge") 
#bank_train %>% ggplot(aes(month,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_test_all %>% ggplot(aes(month,fill=y)) + geom_bar(position="dodge") 
#bank_test_all %>% ggplot(aes(month,fill=y)) + geom_bar(position="fill") + ylab("Proportion")

summary(bank$day_of_week)
bank %>% ggplot(aes(day_of_week,fill=y)) + geom_bar(position="dodge") 
bank %>% ggplot(aes(day_of_week,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
# No differences in day of week

bank %>% ggplot(aes(pdays,fill=y)) + geom_histogram(position="dodge",binwidth=500)
bank %>% filter(pdays<999) %>% ggplot(aes(pdays,stat(density),fill=y)) + geom_histogram(position="dodge") 
# pdays is mostly 999's with a few values < 30
# Possible predictor of yes vs. no, more binary than continuous

# Is pdays=999 the same as poutcome="nonexistent" ?
pdays999=grepl(999,bank$pdays)
summary(pdays999)
poutcome_nonexistent=grepl("nonexistent",bank$poutcome)
summary(poutcome_nonexistent)
diff=factor(abs(pdays999-poutcome_nonexistent),labels=c("same","different"))
summary(diff) 
# Not quite the same, there are 4,110 rows (1% of data) that are different between them
poutcome_999 = bank$poutcome[diff=="different"] 
summary(poutcome_999)
# All 4,110 of the differing rows are poutcome="failure"
summary(bank$poutcome)
# pdays 999 = poutcome "nonexistent" + "failure"
## Confounded Variables
## Keep poutcome for logistic reg.
## Keep pdays for LDA 

summary(bank$education)
# 4% "unknown" values
bank %>% ggplot(aes(education,fill=y)) + geom_bar(position="dodge")
bank %>% ggplot(aes(education,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
length(grep("illiterate",bank$education)) # n=12
#bank_train %>% ggplot(aes(education,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_test_all %>% ggplot(aes(education,fill=y)) + geom_bar(position="fill") + ylab("Proportion")


summary(bank$job)
# <1% "unknown" values
bank %>% ggplot(aes(job,fill=y)) + geom_bar(position="dodge")
bank %>% ggplot(aes(job,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_train %>% ggplot(aes(job,fill=y)) + geom_bar(position="fill") + ylab("Proportion")
#bank_test_all %>% ggplot(aes(job,fill=y)) + geom_bar(position="fill") + ylab("Proportion")

bank %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=11)
bank %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=0.5)
#bank_train %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=10)
#bank_train %>% ggplot(aes(campaign,fill=y)) + geom_histogram(position="fill",binwidth=0.5)

# ?? Recode campaign into ranges: 0-10,10-20,20-30,>40 ??

bank %>% ggplot(aes(previous,fill=y)) + geom_histogram(position="dodge")
bank %>% ggplot(aes(previous,fill=y)) + geom_histogram(position="fill")
#bank_train %>% ggplot(aes(previous,fill=y)) + geom_histogram(position="dodge")
#bank_train %>% ggplot(aes(previous,fill=y)) + geom_histogram(position="fill")


```


```{r EDA Correlations}


# Correlations between continuous predictors

pairs(bank[,c(1,11:14,16:20)],col=bank$y)
# pdays and log_duration (and maybe age or cons._.idx) looks good for LDA
# lots of repeated values for columns 16-20

cor(bank[,c(1,11:14,16:20)])
# emp.var.rate, euribor3m, and nr.employed are highly correlated (r>0.9)
# cons.price.idx is also moderately correlated (r= 0.52 to 0.75) 

```


```{r training-test split}

#Split data by yes's and no's

bank_yes=bank[bank$y=="yes",]
bank_no=bank[bank$y=="no",]


#Split yes's into 70/30 training & test sets

down_sample_size = dim(bank_yes)[1]*0.7

set.seed(1113)
index<-sample(1:dim(bank_yes)[1],down_sample_size,replace=F)
train_yes<-bank_yes[index,]
test_yes<-bank_yes[-index,]

dim(test_yes)[1] #1392 rows

#Take same number of no's as yes's for training set

set.seed(3111)
index<-sample(1:dim(bank_no)[1],down_sample_size,replace=F)
train_no<-bank_no[index,]
test_no<-bank_no[-index,] #All remaining no's in test set

# Recombine the yes's and no's into training and test sets for modeling

bank_train = rbind(train_yes,train_no)
bank_test_all = rbind(test_yes,test_no)


# Rebalance back to 90/10 for test set

no_90 = dim(test_yes)[1]/0.1*0.9
no_90 #12,528 rows

index<-sample(1:dim(test_no)[1],no_90,replace=F)
test_no_90_10 = test_no[index,]

bank_test_90_10 = rbind(test_yes,test_no_90_10)
summary(bank_test_90_10$y)[1]/dim(bank_test_90_10)[1] #check = 90% 


# Rebalance to 50/50 for test set

index<-sample(1:dim(test_no)[1],dim(test_yes)[1],replace=F)
test_no_50_50 = test_no[index,]

bank_test_50_50 = rbind(test_yes,test_no_50_50)


########################################################################
# Use test_50_50 as test set
bank_test=bank_test_90_10

########################################################################

# Check that there are no missing variables between the training and test sets

summary(bank_train)
summary(bank_test)

## job, education, and month all have many levels (>7)

summary(bank_train$job)
summary(bank_test$job)

summary(bank_train$education)
summary(bank_test$education)

summary(bank_train$month)
summary(bank_test$month) 

#Write CSV files for training & test sets

#write.csv(bank_train_50_50,file="~/R/SMU-Stats2-Project2/bank_train_50_50.csv")
#write.csv(bank_test_all,file="~/R/SMU-Stats2-Project2/bank_test_all.csv")
#write.csv(bank_test_90_10,file="~/R/SMU-Stats2-Project2/bank_test_90_10.csv")
#write.csv(bank_test_50_50,file="~/R/SMU-Stats2-Project2/bank_test_50_50.csv")

```


## Check LDA Assumptions before analysis

```{LDA Assumptions}

# Check assumptions of equal covariance for LDA
# Need to center/scale data
# Do these really matter for predictions with training/test sets with large n?

# pdays (13) isn't really a continuous predictor, remove

cov.trob(bank[,c(12,14,16:20)])$cov 
cor(bank[,c(12,4,16:20)])
# nr.employed has high covariance and correlation, remove
# euribor3m is highly correlated with emp.var.rate, remove

cov.trob(bank[,c(12,14,16:18)])$cov
# 3-way combinations looks suspect for equal covariance 
cov.mve(bank[,c(12,16:18)])$cov # scaled and centered
# cons.conf.idx(18) has higher covariances
cor(bank[,c(12,14,16:18)])
# euribor3m highly correlated with emp.var.rate, remove

# all correlation and covariance close to 0
cov.trob(bank[,c(12,16,17)])$cov # centered
cov.mve(bank[,c(12,16,17)])$cov # scaled and centered
cov2cor(cov.mve(bank[,c(12,16,17)])$cov)
cov2cor(cov.trob(bank[,c(12,16,17)])$cov)
cor(bank[,c(12,16,17)])

# all correlation and covariance close to 0
cov.trob(bank[,c(12,16,18)])$cov # centered
cov.mve(bank[,c(12,16,18)])$cov # scaled and centered
cov2cor(cov.mve(bank[,c(12,16,18)])$cov)
cov2cor(cov.trob(bank[,c(12,16,18)])$cov)
cor(bank[,c(12,16,18)])
```


## Simple LDA Model with Full Dataset and Internal CV

```{r Simple LDA}

# Full Data, No Validation

mylda1 <- lda(y ~ campaign + emp.var.rate + cons.price.idx, data = bank)
pred<-predict(mylda1,newdata=bank2)

# ROC Curve
pred<-predict(mylda1,newdata=bank)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,1],bank$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .40, y = .4,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="Simple LDA - Full Data ROC Curve")
# AUC=0.756

# Leave-One_out Internal Cross Validation (CV)

mylda1a <- lda(y ~ campaign + emp.var.rate + cons.price.idx, data = bank, CV=T)

# ROC Curve
preds1 <- as.data.frame(mylda1a$posterior)
pred1 <- prediction(preds1[,1],bank$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .40, y = .4,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="Simple LDA - Full Data ROC Curve")
# AUC=0.747

# Very slightly improved model with Internal CV, Overfit?

mylda1b <- lda(y ~ campaign + emp.var.rate + cons.price.idx + euribor3m + nr.employed, data = bank, CV=T)

# ROC Curve
preds1 <- as.data.frame(mylda1b$posterior)
pred1 <- prediction(preds1[,1],bank$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .40, y = .4,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="Simple LDA - Full Data ROC Curve")
# AUC=0.753

```


## 90/10 Training/Test Import from GitHub 

To be consistent with the group's analysis in both R and SAS, 
import training & test sets from GitHub:

https://github.com/aleppla/SMU-Stats2-Project2/tree/master/Data-Files

```{r import files from GitHub}

bank_train=read.csv("~/R/SMU-Stats2-Project2/bank_train_90_10.csv", header=T)
bank_test=read.csv("~/R/SMU-Stats2-Project2/bank_test_90_10.csv", header=T)
bank_train$y=factor(bank_train$y,levels=c("yes","no"))
bank_test$y=factor(bank_test$y,levels=c("yes","no"))
bank_train=bank_train[,-1]
bank_test=bank_test[,-1]

```


## Simple LDA Model with Down-Sample External CV

```{r Simple LDA 90/10}

## Simple LDA w/ Down-Sampled Training Set

mylda1c <- qda(y ~ campaign + emp.var.rate + cons.price.idx, data = bank_train)

### Training Set

pred<-predict(mylda1c,newdata=bank_train)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,1],bank_train$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .40, y = .5,paste("AUC = ", round(auc.train[[1]],4), sep = ""))
title(main="Simple LDA - Train ROC Curve")
# AUC=0.757 for LDA, 0.760 for QDA


### Test Set - External Validation

pred<-predict(mylda1c,newdata=bank_test)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,1],bank_test$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .40, y = .5,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="Simple LDA - Test ROC Curve")
# AUC=0.754 for LDA, 0.761 for QDA

```


## Objective 2 Model Testing/Development

```{r Obj2 Complex LDA - Internal CV}

# Add 2-way interactions to LDA model
## Full Data Set, Leave-One-Out Interval CV

# Add all cont. predictors and 2-way interactions to LDA model
bank2=bank[,c(1,12:14,16:20,21)]
# previous(14) is the only factor that doesn't improve the model
# age(1) improves model very slightly +0.002 AUC

mylda2 <- qda(y ~.^2, data = bank2,CV=T)

preds1 <- as.data.frame(mylda2$posterior)
pred1 <- prediction(preds1[,1],bank2$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .40, y = .5,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
title(main="LDA Complex Model - Internal CV ROC Curve")
# AUC = 0.780 for LDA, 0.784 for QDA

```


```{r Obj2 Complex LDA - External CV 90/10}

## Training/Test External CV

bank2_train=bank_train[,c(1,12:14,16:20,21)]
bank2_test=bank_test[,c(1,12:14,16:20,21)]

mylda2a <- lda(y ~.^2, data = bank2_train)


# Training Set ROC Curve and Confusion Matrices

pred<-predict(mylda2a,newdata=bank2_train)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,1],bank2_train$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .4, y = .5,paste("AUC = ", round(auc.train[[1]],4), sep = ""))
title(main="LDA Complex Model - Train Set ROC Curve")
# AUC=0.7978


# Test Set ROC Curve and Confusion Matrices 

pred<-predict(mylda2a,newdata=bank2_test)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,1],bank2_test$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .35, y = .5,paste("AUC = ", round(auc.train[[1]],4), sep = ""))
title(main="LDA Complex Model - Test Set ROC Curve")
# AUC=0.7837 for LDA

# Default Threshold = 0.5
confusionMatrix(table(pred$class,bank2_test$y))
# 67% Sens, 80% Spec, 12.5% PPV

# Extract Sens, Spec, and Threshold from ROC Curve
roc.df <- data.frame( unlist(roc.perf@y.values),
                      1-unlist(roc.perf@x.values),
                      unlist(roc.perf@alpha.values) )
names(roc.df)=c("Sensitivity","Specificity","Threshold")

# Balanced Threshold
roc.df$Bal=abs(roc.df$Sensitivity-roc.df$Specificity)
roc.df[roc.df$Bal==min(roc.df$Bal),]
thresh = roc.df$Threshold[roc.df$Bal==min(roc.df$Bal)]
out = factor(ifelse(pred$posterior[,1] > thresh, "yes", "no"), levels = c("yes", "no"))
LDA_balanced_test=confusionMatrix(out,bank2_test$y)
LDA_balanced_test
# Acc=Sens=Spec=PPV=NPV=0.72

# Threshold for Sens=90%
roc.df$Sens90=abs(roc.df$Sensitivity-0.90)
roc.df[roc.df$Sens90==min(roc.df$Sens90),]
thresh = mean(roc.df$Threshold[roc.df$Sens90==min(roc.df$Sens90)])
out <- factor(ifelse(pred$posterior[,1] > thresh, "yes", "no"), levels = c("yes", "no"))
LDA_Sens90_test=confusionMatrix(out,bank2_test$y)
LDA_Sens90_test
# Acc=33%, Sens=90%, Spec=30%, PPV=5%


```


## Objective 2 - QDA

No better than LDA with really low thresholds.

```{r Obj2 QDA 90/10}

## Training/Test External CV

bank2_train=bank_train[,c(1,12:13,17:19,20,21)]
bank2_test=bank_test[,c(1,12:13,17:19,20,21)]

# Training Set

myqda <- qda(y ~.^2, data = bank2_train)

pred<-predict(myqda,newdata=bank2_train)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,1],bank2_train$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .35, y = .5,paste("AUC = ", round(auc.train[[1]],4), sep = ""))
title(main="LDA Complex Model - Train Set ROC Curve")
# AUC=0.7915


# Test Set

pred<-predict(myqda,newdata=bank2_test)
preds1 <- as.data.frame(pred$posterior)
pred1 <- prediction(preds1[,1],bank2_test$y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1,col="blue")
text(x = .35, y = .5,paste("AUC = ", round(auc.train[[1]],4), sep = ""))
title(main="LDA Complex Model - Test Set ROC Curve")
# AUC=0.7829


roc.df <- data.frame( unlist(roc.perf@y.values),
                      1-unlist(roc.perf@x.values),
                      unlist(roc.perf@alpha.values) )
names(roc.df)=c("Sensitivity","Specificity","Threshold")

# Default Threshold = 0.5
CM2_test = confusionMatrix(table(pred$class,bank2_test$y))
CM2_test 
# 67% Sens, 80% Spec, 12.5% PPV

# Balanced Threshold
roc.df$Bal=abs(roc.df$Sensitivity-roc.df$Specificity)
roc.df[roc.df$Bal==min(roc.df$Bal),]
thresh = roc.df$Threshold[roc.df$Bal==min(roc.df$Bal)]
out = factor(ifelse(pred$posterior[,1] > thresh, "yes", "no"), levels = c("yes", "no"))
LDA_balanced_test=confusionMatrix(out,bank2_test$y)
LDA_balanced_test
# Acc=Sens=Spec=PPV=NPV=0.72

```


```{r LDA Priors}

# Just to have the code

# Change priors rather than prediction threshold
mylda2p <- lda(y ~.^2, data = bank2,prior=c(0.33,0.67))
pred<-predict(mylda2p,newdata=bank2)$class
CM2p = confusionMatrix(table(pred,bank2$y))
CM2p

```


```{r Obj1 - Log Reg LASSO 90/10}

#glmnet requires a matrix 
obj1.train.x <- model.matrix(y~.-1,bank_train[,c(-5,-11)]) 
# Exclude duration, a response/covariate vs. a predictor


cvfit1 <- cv.glmnet(obj1.train.x, bank_train$y, family = "binomial", type.measure = "class")
plot(cvfit1)
coef(cvfit1) # Elbow, 6 terms in model
coef(cvfit1, s="lambda.min") # 39 terms

######################################
# Doesn't maintain variable hierarchy
# Rerun model with GLM ?
######################################

#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit1.pred.train <- predict(cvfit1, newx = obj1.train.x, type = "response",s="lambda.min") 

#Create ROC curves
pred <- prediction(1-fit1.pred.train, bank_train$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],4), sep = ""))
title(main="Obj1 - Train Set ROC")
# AUC = 0.7953


#Run model from training set on valid. set 
obj1.test.x = model.matrix(y~.-1,bank_test[,c(-5,-11)])

fit1.pred.test <- predict(cvfit1, newx = obj1.test.x, type = "response",s="lambda.min")

#ROC curves
pred1 <- prediction(1-fit1.pred.test, bank_test$y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],4), sep = ""))
title(main="Obj1 - Test Set ROC")
# AUC = 0.7878

roc.df <- data.frame( unlist(roc.perf1@y.values),
                      1-unlist(roc.perf1@x.values),
                      unlist(roc.perf1@alpha.values) )
names(roc.df)=c("Sensitivity","Specificity","Threshold")

roc.df$Bal=abs(roc.df$Sensitivity-roc.df$Specificity)
roc.df[roc.df$Bal==min(roc.df$Bal),]
thresh = roc.df$Threshold[roc.df$Bal==min(roc.df$Bal)]
out = factor(ifelse(1-fit1.pred.test > thresh, "yes", "no"), levels = c("yes", "no"))
LASSO_balanced_test=confusionMatrix(out,bank_test$y)
LASSO_balanced_test

```


## Fit the LASSO model with GLM to get AIC, VIFs, etc.

```{r GLM Logistic 90/10}

model.main<-glm(y~job+marital+education+housing+contact+month+day_of_week+campaign+pdays+poutcome+emp.var.rate+cons.price.idx+cons.conf.idx+nr.employed, bank_train, family = binomial(link="logit"))

#model.main<-glm(y~contact+month+pdays+poutcome+emp.var.rate+nr.employed, bank_train, family = binomial(link="logit"))

summary(model.main)

(vif(model.main)[,3])^2
#emp.var.rate >> 10 VIF


hoslem.test(model.main$y, fitted(model.main), g=10)
# p=0.075

fit1.pred.train <- predict(model.main, newdata = bank_train, type = "response") 

#Create ROC curves
pred <- prediction(1-fit1.pred.train, bank_train$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],4), sep = ""))
title(main="Train Set ROC")
# AUC = 0.7925

#Run model from training set on valid set 
fit1.pred.test <- predict(model.main, newdata = bank_test, type = "response")

#ROC curves
pred1 <- prediction(1-fit1.pred.test, bank_test$y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],4), sep = ""))
title(main="Test Set ROC")
# AUC = 0.784

roc.df <- data.frame( unlist(roc.perf1@y.values),
                      1-unlist(roc.perf1@x.values),
                      unlist(roc.perf1@alpha.values) )
names(roc.df)=c("Sensitivity","Specificity","Threshold")

roc.df$Bal=abs(roc.df$Sensitivity-roc.df$Specificity)
roc.df[roc.df$Bal==min(roc.df$Bal),]
thresh = roc.df$Threshold[roc.df$Bal==min(roc.df$Bal)]
out = factor(ifelse(1-fit1.pred.test > thresh, "yes", "no"), levels = c("yes", "no"))
LASSO_balanced_test=confusionMatrix(out,bank_test$y)
LASSO_balanced_test

```


## Fit a more complex Logistic model with interaction terms 
 
Reduce training set size to 50/50 for faster compute times.

### 50/50 Training Set Import from GitHub 

To be consistent with the group's analysis in both R and SAS, 
import training & test sets from GitHub:

https://github.com/aleppla/SMU-Stats2-Project2/tree/master/Data-Files

```{r import files from GitHub}
bank_train=read.csv("~/R/SMU-Stats2-Project2/bank_train_50_50.csv", header=T)
bank_train$y=factor(bank_train$y,levels=c("yes","no"))
bank_train=bank_train[,-1]

```


```{r LASSO with Interactions 50/50}

#glmnet requires a matrix 
dat.train.x <- model.matrix(y~.^2-1,bank_train[,c(-4,-5,-10,-11)])

#model.main<-glm(y~contact+month+pdays+poutcome+emp.var.rate+nr.employed, bank_train, family = binomial(link="logit"))
#bank_train2=bank_train[,c(8,9,13,15,16,20,21)]
#dat.train.x <- model.matrix(y~.-1,bank_train2)

cvfit <- cv.glmnet(dat.train.x, bank_train$y, family = "binomial", type.measure = "class", nlambda = 50)

plot(cvfit)
coef(cvfit) # 20/350 terms in model
coef(cvfit,s="lambda.min")

#####################################
# Doesn't maintain variable hierarchy
#####################################

#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit.pred <- predict(cvfit, newx = dat.train.x, type = "response",s="lambda.min") #
#Create ROC curves
pred <- prediction(1-fit.pred, bank_train$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],4), sep = ""))
title(main="Obj2 Training Set ROC")
# AUC = 0.7993, marginally better than simple model

#Run model from training set on valid set 
#bank_test2=bank_test[,c(-4,-5,-10,-11)]
#dat.test.x = model.matrix(y~.-1,bank_test2)

dat.test.x <- model.matrix(y~.-1,bank_test[,c(-4,-5,-10,-11)])

fit.pred1 <- predict(cvfit, newx = dat.test.x, type = "response",s="lambda.min") #

#ROC curves
pred1 <- prediction(1-fit.pred1, bank_test$y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],4), sep = ""))
title(main="Obj2 Test Set ROC")
# AUC = 0.7849, marginally better than simple model
```





## Dr. Turner's code for plotting multiple model AUC curves

```{r Compare}

#Here we are just comparing the reproducibility of the logistic model.
#You can also use this code to compare multiple models like logistic, compared to lda, compared to a tree or RF.
#This graph also allows for you to get a sense of what cut off values are producing the best sensitivity and specificity results as well using
#the colorize option.

#This is helpful:  https://www.r-bloggers.com/a-small-introduction-to-the-rocr-package/ that does some extra things you might find 
#helpful.
#If you want to mess around with other packages: https://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/
plot( roc.perf1, colorize = TRUE)
plot(roc.perf2, add = TRUE, colorize = TRUE)
plot(roc.perf3, add = TRUE, colorize = TRUE)
abline(a=0, b= 1)

#without color for cutoff; but adding colors to allow for comarisons of the curves
plot( roc.perf1)
plot(roc.perf2,col="orange", add = TRUE)
plot(roc.perf3,col="blue", add = TRUE)
legend("bottomright",legend=c("Valid 1","Valid 2","Valid 3"),col=c("black","orange","blue"),lty=1,lwd=1)
abline(a=0, b= 1)


```



```{r Randon Forest}
#Random forrest
#Here we will do a truly RF run by selecting mtry. mtry controls how many
#predictors are sampled for each bootstrap sample.
rf<-randomForest(y~.-duration,data=bank_train,mtry=9,importance=T,ntree=1000)


#Go get the ROC
rf.pred<-predict(rf,newdata=bank_test,type="prob")
pred <- prediction(rf.pred[,1], bank_test$y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
#Note in the following code the term "train" means nothing here. 
#I'm just rinsing and repeating code the produces the curve.
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf,main="AUC of Test set RF - mtry=9")
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],4), sep = ""))


#Making predictions on test and then observing accuracy rates
fit.pred<-predict(rf,newdata=bank_test,type="response",cutoff=c(0.58,0.42))
confusionMatrix(fit.pred,bank_test$y) #Default prediction uses .5 as cut off you can change it specifying "cutoff" option


#Which variables are important.  We can use variable importance.

varImpPlot (rf,type=1,main="Variable Importance")
varImpPlot (rf,type=2,main="Variable Importance")
```
